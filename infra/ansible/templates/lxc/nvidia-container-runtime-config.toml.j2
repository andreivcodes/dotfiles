# NVIDIA Container Runtime Configuration for LXC
# Generated by Ansible for {{ lxc_hostname }} (ID: {{ lxc_id }})

disable-require = false
#swarm-resource = "DOCKER_RESOURCE_GPU"
#accept-nvidia-visible-devices-envvar-when-unprivileged = true
#accept-nvidia-visible-devices-as-volume-mounts = false

[nvidia-container-cli]
#root = "/run/nvidia/driver"
#path = "/usr/bin/nvidia-container-cli"
environment = []
#debug = "/var/log/nvidia-container-toolkit.log"
#ldcache = "/etc/ld.so.cache"
load-kmods = true
# CRITICAL: Set to true for LXC containers - this disables cgroup device management
no-cgroups = true
#user = "root:video"
ldconfig = "@/sbin/ldconfig.real"

[nvidia-container-runtime]
#debug = "/var/log/nvidia-container-runtime.log"
# Optionally image could be config at runtime
# log-driver = "json-file"
# log-level = "info"

[nvidia-container-runtime.modes]

[nvidia-container-runtime.modes.csv]
mount-spec-path = "/etc/nvidia-container-runtime/host-files-for-container.d"

[nvidia-container-runtime.modes.cdi]
#default-kind = "nvidia.com/gpu"
#annotation-prefixes = ["cdi.k8s.io/"]
#spec-dirs = ["/etc/cdi", "/var/run/cdi"]

[nvidia-ctk]
#path = "/usr/bin/nvidia-ctk"

[nvidia-ctk.detail]
#environment = ["PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"]