---
- name: Create Debian LXC with GPU+TUN and install Tailscale, Docker, and Dokploy
  hosts: proxmox
  become: true
  vars_files:
    - ../../group_vars/all/main.yml
    - ../../group_vars/all/vault.yml

  vars:
    # Override at runtime via ANSIBLE_ARGS if desired
    lxc_vmid: 111
    lxc_hostname: proposalsapp-infra
    # Debian 12.7-1 template (same as ollama)
    lxc_ostemplate: "local:vztmpl/debian-12-standard_12.7-1_amd64.tar.zst"
    # Use SSD pool for CT rootfs
    lxc_storage: ssd-raid
    lxc_memory_mb: 8192
    lxc_cores: 8
    lxc_disk_gb: 100
    # Use SDN vnet bridge (created by SDN playbooks)
    lxc_bridge: sibnet
    lxc_ip: 192.168.100.111/24
    lxc_gw: 192.168.100.1
    # Dokploy specific settings
    dokploy_port: 3000
    # Use privileged container for better Docker compatibility (can be changed to unprivileged with more config)
    lxc_unprivileged: 0
    # Enable GPU support for AI/ML workloads deployed through Dokploy
    enable_gpu_passthrough: true

  tasks:
    - name: Compute template filename
      tags: [create]
      set_fact:
        lxc_template_filename: "{{ lxc_ostemplate.split(':')[-1] | regex_replace('^vztmpl/', '') }}"

    - name: Check if Debian template is already downloaded
      tags: [create]
      stat:
        path: "/var/lib/vz/template/cache/{{ lxc_template_filename }}"
      register: lxc_template_stat

    - name: Update template catalog
      tags: [create]
      command: pveam update
      when: not lxc_template_stat.stat.exists
      changed_when: true

    - name: Download Debian template to local storage (exact name)
      tags: [create]
      command: pveam download local {{ lxc_template_filename }}
      when: not lxc_template_stat.stat.exists
      register: template_download_attempt
      failed_when: false
      changed_when: template_download_attempt.rc == 0

    - name: Find latest available Debian 12 standard template (fallback)
      tags: [create]
      shell: |
        pveam available | awk '{print $2}' | \
        grep '^debian-12-standard_.*_amd64.tar.\(zst\|gz\)$' | \
        sort -Vr | head -n1
      register: deb12_latest
      when: not lxc_template_stat.stat.exists and (template_download_attempt is defined and template_download_attempt.rc != 0)
      changed_when: false

    - name: Download Debian template to local storage (fallback to latest)
      tags: [create]
      command: pveam download local {{ deb12_latest.stdout }}
      when: not lxc_template_stat.stat.exists and (template_download_attempt is defined and template_download_attempt.rc != 0) and (deb12_latest.stdout | length) > 0
      register: template_download_fallback
      changed_when: template_download_fallback.rc == 0

    - name: Create LXC container (idempotent) - privileged for Docker Swarm compatibility
      tags: [create]
      shell: >-
        pct create {{ lxc_vmid }} {{ lxc_ostemplate }}
        --hostname {{ lxc_hostname }}
        --unprivileged {{ lxc_unprivileged }}
        --memory {{ lxc_memory_mb }}
        --cores {{ lxc_cores }}
        --features nesting=1,keyctl=1
        --net0 name=eth0,bridge={{ lxc_bridge }},ip={{ lxc_ip }},gw={{ lxc_gw }}
        --rootfs {{ lxc_storage }}:{{ lxc_disk_gb }}
      args:
        creates: "/etc/pve/lxc/{{ lxc_vmid }}.conf"

    - name: Configure LXC for Docker compatibility
      tags: [create]
      ansible.builtin.blockinfile:
        path: "/etc/pve/lxc/{{ lxc_vmid }}.conf"
        marker: "# {mark} ANSIBLE DOCKER CONFIG"
        block: |
          # Docker compatibility settings
          lxc.apparmor.profile: unconfined
          lxc.cgroup2.devices.allow: a
          lxc.cap.drop:
          lxc.mount.auto: proc:rw sys:rw

    - name: Configure TUN device for Tailscale
      tags: [create]
      ansible.builtin.blockinfile:
        path: "/etc/pve/lxc/{{ lxc_vmid }}.conf"
        marker: "# {mark} ANSIBLE TUN CONFIG"
        block: |
          # TUN device for Tailscale
          lxc.cgroup2.devices.allow: c 10:200 rwm
          lxc.mount.entry: /dev/net/tun dev/net/tun none bind,create=file

    - name: Configure GPU passthrough if enabled
      tags: [create]
      ansible.builtin.blockinfile:
        path: "/etc/pve/lxc/{{ lxc_vmid }}.conf"
        marker: "# {mark} ANSIBLE NVIDIA CONFIG"
        block: |
          # NVIDIA GPU passthrough
          lxc.cgroup2.devices.allow: c 195:* rwm
          lxc.cgroup2.devices.allow: c 234:* rwm
          lxc.cgroup2.devices.allow: c 508:* rwm
          lxc.cgroup2.devices.allow: c 509:* rwm
          lxc.mount.entry: /dev/nvidia0 dev/nvidia0 none bind,optional,create=file
          lxc.mount.entry: /dev/nvidia1 dev/nvidia1 none bind,optional,create=file
          lxc.mount.entry: /dev/nvidiactl dev/nvidiactl none bind,optional,create=file
          lxc.mount.entry: /dev/nvidia-modeset dev/nvidia-modeset none bind,optional,create=file
          lxc.mount.entry: /dev/nvidia-uvm dev/nvidia-uvm none bind,optional,create=file
          lxc.mount.entry: /dev/nvidia-uvm-tools dev/nvidia-uvm-tools none bind,optional,create=file
          lxc.mount.entry: /dev/nvidia-caps dev/nvidia-caps none bind,optional,create=dir
      when: enable_gpu_passthrough

    - name: Start container
      tags: [create]
      command: pct start {{ lxc_vmid }}
      register: start_ct
      failed_when: false
      changed_when: start_ct.rc == 0

    - name: Wait for LXC init to settle
      tags: [create]
      command: pct status {{ lxc_vmid }}
      register: lxc_status
      retries: 10
      delay: 2
      until: "lxc_status.rc == 0 and (lxc_status.stdout is search('status: running'))"

    - name: Update apt cache inside LXC
      tags: [configure]
      command: pct exec {{ lxc_vmid }} -- bash -lc 'apt-get update'
      retries: 3
      delay: 5

    - name: Install curl first (required for most installers)
      tags: [configure]
      command: >-
        pct exec {{ lxc_vmid }} -- bash -lc
        "apt-get install -y curl ca-certificates gnupg"
      retries: 3
      delay: 5

    - name: Configure kernel parameters for Docker
      tags: [configure]
      command: >-
        pct exec {{ lxc_vmid }} -- bash -lc
        "echo 'net.ipv4.ip_forward=1' >> /etc/sysctl.conf &&
         echo 'net.bridge.bridge-nf-call-iptables=1' >> /etc/sysctl.conf &&
         echo 'net.bridge.bridge-nf-call-ip6tables=1' >> /etc/sysctl.conf &&
         sysctl -p"
      failed_when: false

    - name: Install Tailscale inside LXC
      tags: [configure]
      command: >-
        pct exec {{ lxc_vmid }} -- bash -lc
        "curl -fsSL https://tailscale.com/install.sh | sh"

    - name: Enable and start tailscaled inside LXC
      tags: [configure]
      command: pct exec {{ lxc_vmid }} -- bash -lc 'systemctl enable --now tailscaled'

    - name: Determine if we can authenticate Tailscale
      tags: [configure]
      set_fact:
        ts_can_auth: "{{ (tailscale_auth_key is defined and (tailscale_auth_key | string | length) > 0) or (tailscale_api_token is defined and (tailscale_api_token | string | length) > 0) }}"

    - name: Fail early if no way to authenticate to Tailscale
      tags: [configure]
      fail:
        msg: >-
          No Tailscale auth available. Provide 'tailscale_auth_key' or 'tailscale_api_token'
          via vault/group_vars to allow automatic login and approval.
      when: not ts_can_auth

    - name: Ensure Tailscale ACL policy allows tag:proxmox (align with proxmox setup)
      tags: [configure]
      uri:
        url: "https://api.tailscale.com/api/v2/tailnet/-/acl"
        method: POST
        headers:
          Authorization: "Bearer {{ tailscale_api_token }}"
          Content-Type: application/json
          Accept: application/json
        body_format: json
        body:
          acls:
            - action: accept
              src: ["*"]
              dst: ["*:22", "*:8006", "*:8007", "*:3128", "*:3000", "*:80", "*:443"]
            - action: accept
              src: ["tag:proxmox"]
              dst: ["*:*"]
          tagOwners:
            tag:proxmox: ["autogroup:admin"]
        return_content: yes
        status_code: [200, 201]
      register: ts_acl_update_ct
      delegate_to: localhost
      become: false
      changed_when: ts_acl_update_ct.status in [200, 201]
      when:
        - tailscale_api_token is defined and (tailscale_api_token | string | length) > 0

    - name: Query Tailscale devices (pre-clean existing entries for CT hostname)
      tags: [configure]
      uri:
        url: "https://api.tailscale.com/api/v2/tailnet/-/devices"
        method: GET
        headers:
          Authorization: "Bearer {{ tailscale_api_token }}"
          Accept: application/json
        return_content: yes
        status_code: 200
      register: ts_devices_ct_pre
      no_log: true
      delegate_to: localhost
      become: false
      when: tailscale_api_token is defined and (tailscale_api_token | string | length) > 0

    - name: Build duplicate CT devices by hostname
      tags: [configure]
      set_fact:
        ts_duplicates_ct: >-
          {{ ((ts_devices_ct_pre.json.devices | default(ts_devices_ct_pre.json)) | default([]))
             | selectattr('hostname','equalto', lxc_hostname) | list
             +
             ((ts_devices_ct_pre.json.devices | default(ts_devices_ct_pre.json)) | default([]))
             | selectattr('name','equalto', lxc_hostname) | list }}
      when: ts_devices_ct_pre is defined and ts_devices_ct_pre.json is defined
      delegate_to: localhost
      become: false

    - name: Remove duplicate CT devices
      tags: [configure]
      uri:
        url: "https://api.tailscale.com/api/v2/device/{{ item.id }}"
        method: DELETE
        headers:
          Authorization: "Bearer {{ tailscale_api_token }}"
        status_code: [200, 204, 404]
      loop: "{{ ts_duplicates_ct | default([]) }}"
      loop_control:
        label: "{{ item.id }} {{ item.hostname | default(item.name) }}"
      register: ts_ct_delete_results
      no_log: true
      delegate_to: localhost
      become: false
      when:
        - ts_duplicates_ct is defined
        - (ts_duplicates_ct | length) > 0
        - tailscale_api_token is defined and (tailscale_api_token | string | length) > 0

    - name: Always create fresh preauthorized Tailscale auth key for CT
      tags: [configure]
      uri:
        url: "https://api.tailscale.com/api/v2/tailnet/-/keys"
        method: POST
        headers:
          Authorization: "Bearer {{ tailscale_api_token }}"
          Content-Type: application/json
          Accept: application/json
        body_format: json
        body:
          capabilities:
            devices:
              create:
                reusable: true
                ephemeral: false
                preauthorized: true
                tags:
                  - "tag:proxmox"
          description: "ct-dokploy-{{ lxc_hostname }}-{{ lxc_vmid }}"
          expirySeconds: 86400
        return_content: true
        status_code: [200, 201]
      register: ts_key_create_ct
      no_log: true
      delegate_to: localhost
      become: false
      when: (tailscale_api_token is defined and (tailscale_api_token | string | length) > 0)

    - name: Choose Tailscale auth key for CT
      tags: [configure]
      set_fact:
        ts_auth_key_to_use: "{{ (ts_key_create_ct.json.key | default('')) if (ts_key_create_ct is defined and ts_key_create_ct.json is defined) else (tailscale_auth_key | default('')) }}"
      no_log: true

    - name: Configure Tailscale to use userspace networking (avoid conflicts with Docker)
      tags: [configure]
      command: >-
        pct exec {{ lxc_vmid }} -- bash -lc
        "mkdir -p /etc/default && echo 'TS_USERSPACE=true' >> /etc/default/tailscaled"

    - name: Tailscale up with auth key (idempotent)
      tags: [configure]
      command: >-
        pct exec {{ lxc_vmid }} -- bash -lc
        "(test -n '{{ ts_auth_key_to_use | default(tailscale_auth_key) }}' && tailscale up --authkey={{ ts_auth_key_to_use | default(tailscale_auth_key) }} --accept-routes --hostname={{ lxc_hostname }} --reset) || true"
      no_log: true
      failed_when: false

    - name: Check Tailscale connection status in CT (initial)
      tags: [configure]
      command: pct exec {{ lxc_vmid }} -- bash -lc 'tailscale status 2>&1 || true'
      register: ts_status_once
      failed_when: false
      changed_when: false

    - name: Fetch devices from Tailscale API (for approval)
      tags: [configure]
      uri:
        url: "https://api.tailscale.com/api/v2/tailnet/-/devices"
        method: GET
        headers:
          Authorization: "Bearer {{ tailscale_api_token }}"
          Accept: application/json
        return_content: yes
        status_code: 200
      register: ts_devices_for_approval_ct
      no_log: true
      delegate_to: localhost
      become: false
      when:
        - tailscale_api_token is defined and (tailscale_api_token | string | length) > 0
        - ts_status_once.stdout is defined
        - ts_status_once.stdout is search('Machine is not yet approved')

    - name: Extract CT device ID from Tailscale devices
      tags: [configure]
      set_fact:
        ct_device_id_extracted: "{{ (ts_devices_for_approval_ct.json.devices | selectattr('hostname', 'equalto', lxc_hostname) | list | first).id | default('') }}"
      when:
        - ts_devices_for_approval_ct is defined
        - ts_devices_for_approval_ct.json is defined
        - ts_devices_for_approval_ct.json.devices is defined

    - name: Approve CT device via Tailscale API
      tags: [configure]
      uri:
        url: "https://api.tailscale.com/api/v2/device/{{ ct_device_id_extracted }}/authorized"
        method: POST
        headers:
          Authorization: "Bearer {{ tailscale_api_token }}"
          Content-Type: application/json
        body_format: json
        body:
          authorized: true
        status_code: [200, 201]
      no_log: true
      delegate_to: localhost
      become: false
      when:
        - ct_device_id_extracted is defined
        - ct_device_id_extracted | length > 0

    - name: Wait for Tailscale to connect after approval/login
      tags: [configure]
      command: pct exec {{ lxc_vmid }} -- bash -lc 'tailscale status 2>&1 || true'
      register: ts_status_wait
      retries: 20
      delay: 3
      until: "(ts_status_wait.stdout is defined) and (not (ts_status_wait.stdout is search('Machine is not yet approved'))) and (not (ts_status_wait.stdout is search('NeedsLogin')))"
      changed_when: false

    - name: Wait for CT to get Tailscale IP (IPv4)
      tags: [configure]
      command: pct exec {{ lxc_vmid }} -- bash -lc 'tailscale ip -4 | head -n1'
      register: ct_ts_ip
      retries: 20
      delay: 3
      until: "ct_ts_ip.rc == 0 and (ct_ts_ip.stdout | trim | length) > 0"
      changed_when: false

    - name: Configure Tailscale tags for CT (match proxmox tag usage)
      tags: [configure]
      command: >-
        pct exec {{ lxc_vmid }} -- bash -lc
        "tailscale up --advertise-tags=tag:proxmox --hostname={{ lxc_hostname }} || true"
      register: ts_tag_config
      failed_when: false
      changed_when: ts_tag_config.rc == 0

    - name: Install Docker inside LXC (official script)
      tags: [configure]
      command: pct exec {{ lxc_vmid }} -- bash -lc 'curl -fsSL https://get.docker.com | sh'

    - name: Enable and start Docker inside LXC
      tags: [configure]
      command: pct exec {{ lxc_vmid }} -- bash -lc 'systemctl enable --now docker'

    # Docker daemon config removed - using defaults for LXC compatibility

    - name: Wait for Docker to be ready
      tags: [configure]
      command: pct exec {{ lxc_vmid }} -- bash -lc 'docker ps'
      register: docker_ready
      retries: 10
      delay: 3
      until: "docker_ready.rc == 0"

    - name: Check for NVIDIA devices in container (if GPU enabled)
      tags: [configure]
      command: pct exec {{ lxc_vmid }} -- bash -lc 'ls -la /dev/nvidia* 2>/dev/null || echo "No NVIDIA devices found"'
      register: nvidia_devices_check
      failed_when: false
      changed_when: false
      when: enable_gpu_passthrough

    # No longer need to get specific driver version - will install latest from repository

    - name: Add NVIDIA CUDA repository in container
      tags: [configure]
      command: >-
        pct exec {{ lxc_vmid }} -- bash -lc
        "wget -qO /tmp/cuda-keyring.deb https://developer.download.nvidia.com/compute/cuda/repos/debian12/x86_64/cuda-keyring_1.1-1_all.deb &&
         dpkg -i /tmp/cuda-keyring.deb &&
         apt-get update"
      register: cuda_repo_added
      failed_when: false
      changed_when: cuda_repo_added.rc == 0
      when: enable_gpu_passthrough

    - name: Install NVIDIA driver libraries in container
      tags: [configure]
      command: >-
        pct exec {{ lxc_vmid }} -- bash -lc
        "DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends cuda-drivers"
      register: nvidia_install
      failed_when: false
      changed_when: nvidia_install.rc == 0
      when: enable_gpu_passthrough

    - name: Install NVIDIA Container Toolkit inside LXC (if GPU enabled)
      tags: [configure]
      command: >-
        pct exec {{ lxc_vmid }} -- bash -lc
        "curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey > /tmp/nvidia.gpg &&
        cat /tmp/nvidia.gpg | gpg --batch --yes --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg &&
        curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list |
        sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' |
        tee /etc/apt/sources.list.d/nvidia-container-toolkit.list &&
        apt-get update &&
        apt-get install -y nvidia-container-toolkit"
      when: enable_gpu_passthrough

    - name: Configure NVIDIA Container Runtime to disable cgroups for LXC
      tags: [configure]
      command: >-
        pct exec {{ lxc_vmid }} -- bash -lc
        "mkdir -p /etc/nvidia-container-runtime &&
        echo '[nvidia-container-cli]' > /etc/nvidia-container-runtime/config.toml &&
        echo 'no-cgroups = true' >> /etc/nvidia-container-runtime/config.toml &&
        echo '[nvidia-container-runtime]' >> /etc/nvidia-container-runtime/config.toml"
      when: enable_gpu_passthrough

    - name: Configure Docker to use NVIDIA runtime
      tags: [configure]
      command: >-
        pct exec {{ lxc_vmid }} -- bash -lc
        "nvidia-ctk runtime configure --runtime=docker &&
        systemctl restart docker"
      when: enable_gpu_passthrough

    - name: Create working directory for Dokploy
      tags: [configure]
      command: pct exec {{ lxc_vmid }} -- bash -lc 'mkdir -p /root/dokploy && cd /root/dokploy'

    - name: Install Dokploy using official script (with workarounds for LXC)
      tags: [configure]
      command: >-
        pct exec {{ lxc_vmid }} -- bash -lc
        "export DOKPLOY_DOCKER_NETWORK=dokploy-network &&
         export DISABLE_SWARM=true &&
         curl -sSL https://dokploy.com/install.sh | sh"
      register: dokploy_install
      failed_when: false

    - name: Check if Dokploy is running
      tags: [configure]
      command: pct exec {{ lxc_vmid }} -- bash -lc 'docker ps | grep dokploy'
      register: dokploy_status
      failed_when: false
      changed_when: false

    - name: Manual Dokploy installation if script failed (common in LXC)
      tags: [configure]
      when: dokploy_install.rc != 0 or dokploy_status.rc != 0
      block:
        - name: Pull Dokploy Docker image
          command: pct exec {{ lxc_vmid }} -- bash -lc 'docker pull dokploy/dokploy:latest'

        - name: Create Dokploy network
          command: pct exec {{ lxc_vmid }} -- bash -lc 'docker network create dokploy-network 2>/dev/null || true'

        - name: Create Dokploy volumes
          command: >-
            pct exec {{ lxc_vmid }} -- bash -lc
            "docker volume create dokploy-data &&
             docker volume create dokploy-docker-config"

        - name: Run Dokploy container with proper configuration
          command: >-
            pct exec {{ lxc_vmid }} -- bash -lc
            "docker run -d \
              --name dokploy \
              --hostname dokploy \
              --restart unless-stopped \
              --network dokploy-network \
              -p {{ dokploy_port }}:3000 \
              -v /var/run/docker.sock:/var/run/docker.sock \
              -v dokploy-data:/data \
              -v dokploy-docker-config:/root/.docker \
              -e DOCKER_HOST=unix:///var/run/docker.sock \
              dokploy/dokploy:latest"

    - name: Wait for Dokploy to be ready
      tags: [configure]
      command: pct exec {{ lxc_vmid }} -- bash -lc 'curl -fsS http://127.0.0.1:{{ dokploy_port }}/'
      register: dokploy_ready
      retries: 30
      delay: 5
      until: "dokploy_ready.rc == 0"

    - name: Configure firewall rules for Dokploy access via Tailscale
      tags: [configure]
      command: >-
        pct exec {{ lxc_vmid }} -- bash -lc
        "iptables -C INPUT -i tailscale0 -p tcp --dport {{ dokploy_port }} -j ACCEPT 2>/dev/null || 
         iptables -A INPUT -i tailscale0 -p tcp --dport {{ dokploy_port }} -j ACCEPT &&
         iptables -C INPUT -i tailscale0 -p tcp --dport 80 -j ACCEPT 2>/dev/null || 
         iptables -A INPUT -i tailscale0 -p tcp --dport 80 -j ACCEPT &&
         iptables -C INPUT -i tailscale0 -p tcp --dport 443 -j ACCEPT 2>/dev/null || 
         iptables -A INPUT -i tailscale0 -p tcp --dport 443 -j ACCEPT"

    - name: Install iptables-persistent to save firewall rules
      tags: [configure]
      command: >-
        pct exec {{ lxc_vmid }} -- bash -lc
        "DEBIAN_FRONTEND=noninteractive apt-get install -y iptables-persistent &&
         netfilter-persistent save"

    - name: Create systemd service to fix Docker networking after reboot
      tags: [configure]
      command: >-
        pct exec {{ lxc_vmid }} -- bash -lc
        "cat > /etc/systemd/system/docker-network-fix.service << 'EOF'
         [Unit]
         Description=Fix Docker networking in LXC
         After=docker.service
         Requires=docker.service
         
         [Service]
         Type=oneshot
         ExecStartPre=/bin/sleep 5
         ExecStart=/bin/bash -c 'sysctl -w net.ipv4.ip_forward=1'
         RemainAfterExit=yes
         
         [Install]
         WantedBy=multi-user.target
         EOF"

    - name: Enable Docker network fix service
      tags: [configure]
      command: pct exec {{ lxc_vmid }} -- bash -lc 'systemctl daemon-reload && systemctl enable docker-network-fix.service'

    - name: Test Dokploy access from Proxmox host over Tailscale
      tags: [configure]
      uri:
        url: "http://{{ (ct_ts_ip.stdout_lines | default([])) | first }}:{{ dokploy_port }}/"
        method: GET
        return_content: true
        status_code: 200
      register: dokploy_access_test
      failed_when: false
      changed_when: false

    - name: Show Dokploy access information
      tags: [configure]
      command: >-
        pct exec {{ lxc_vmid }} -- bash -lc
        "echo '================================';
         echo 'Dokploy Installation Complete!';
         echo '================================';
         echo '';
         echo 'Tailscale IP:'; tailscale ip -4 || true;
         echo '';
         echo 'Docker Status:'; docker ps | grep dokploy || true;
         echo '';
         {% if enable_gpu_passthrough %}
         echo 'GPU Status:'; docker run --rm --runtime=nvidia --gpus all nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi;
         echo '';
         {% endif %}
         echo 'Access URLs:';
         echo '  Dokploy Dashboard: http://$(tailscale ip -4):{{ dokploy_port }}';
         echo '';
         echo 'Initial Setup:';
         echo '  1. Navigate to the dashboard URL above';
         echo '  2. Create your admin account';
         echo '  3. Start deploying your applications!';
         echo '';
         echo 'Notes:';
         echo '  - Docker Swarm is disabled to avoid LXC compatibility issues';
         echo '  - Single-node deployment mode is configured';
         echo '  - Traefik will handle routing for deployed apps';
         echo '  - GPU support is {% if enable_gpu_passthrough %}enabled{% else %}disabled{% endif %}';
         echo '================================'"

    - name: Create helper script for Dokploy management
      tags: [configure]
      command: >-
        pct exec {{ lxc_vmid }} -- bash -lc
        "cat > /usr/local/bin/dokploy-status << 'EOF'
         #!/bin/bash
         echo '=== Dokploy Status ==='
         echo ''
         echo 'Container Status:'
         docker ps | grep dokploy
         echo ''
         echo 'Logs (last 20 lines):'
         docker logs dokploy --tail 20
         echo ''
         echo 'Network:'
         docker network ls | grep dokploy
         echo ''
         echo 'Volumes:'
         docker volume ls | grep dokploy
         echo ''
         echo 'Access URL:'
         echo \"http://$(tailscale ip -4):{{ dokploy_port }}\"
         EOF
         && chmod +x /usr/local/bin/dokploy-status"

    - name: Display important notes
      tags: [configure]
      debug:
        msg:
          - "Dokploy LXC created successfully!"
          - "VMID: {{ lxc_vmid }}"
          - "Hostname: {{ lxc_hostname }}"
          - "Resources: {{ lxc_cores }} cores, {{ lxc_memory_mb }}MB RAM, {{ lxc_disk_gb }}GB disk"
          - ""
          - "IMPORTANT NOTES:"
          - "1. This is a privileged container for Docker compatibility"
          - "2. Docker Swarm is disabled - single-node deployment only"
          - "3. Access via Tailscale network only for security"
          - "4. Run 'dokploy-status' inside container to check status"
          - ""
          - "Known Issues Addressed:"
          - "- Used privileged container to avoid Docker permission issues"
          - "- Disabled Swarm mode to avoid port 2377 problems"
          - "- Configured custom Docker network to avoid conflicts"
          - "- Added network fix service for reboot persistence"