---
- name: Ollama LXC Container Setup with GPU Support
  hosts: proxmox
  gather_facts: true
  become: true
  vars_files:
    - ../../group_vars/all/main.yml
    - ../../group_vars/all/vault.yml
    
  vars:
    lxc_id: 300
    lxc_hostname: ollama
    lxc_template: "debian-12-standard_12.7-1_amd64"
    lxc_storage: "ssd-raid"
    lxc_disk_size: 100
    lxc_cores: 8
    lxc_memory: 16384  # 16GB
    lxc_network: "net0 name=eth0,bridge=sibnet,ip=192.168.100.210/24,gw=192.168.100.1"  # sibnet SDN network with fixed IP
    lxc_features: "nesting=1,keyctl=1"
    docker_compose_dir: "/opt/ollama"
    
  tags:
    - lxc
    - ollama
    - gpu
    - docker

  tasks:
    - name: Check if NVIDIA GPUs are available on host
      shell: lspci | grep -i nvidia
      register: nvidia_gpus
      changed_when: false
      failed_when: false
      
    - name: Fail if no NVIDIA GPUs found
      fail:
        msg: "No NVIDIA GPUs found. Ensure GPU setup playbook has been run first."
      when: nvidia_gpus.rc != 0
    
    - name: Check if nvidia-smi is working on host
      command: nvidia-smi
      register: nvidia_smi_check
      failed_when: nvidia_smi_check.rc != 0
      changed_when: false
    
    - name: Display GPU information
      debug:
        msg: 
          - "NVIDIA GPUs found:"
          - "{{ nvidia_gpus.stdout_lines }}"

    # Template management
    - name: Check available templates
      command: pveam list local
      register: available_templates
      changed_when: false
      failed_when: false
      
    - name: Update template list if needed
      command: pveam update
      when: lxc_template not in available_templates.stdout
      register: template_update
      
    - name: Download Debian template if not present
      command: "pveam download local {{ lxc_template }}.tar.zst"
      when: lxc_template not in available_templates.stdout
      register: template_download
      failed_when: template_download.rc != 0 and "already exists" not in template_download.stderr
      
    # Container creation
    - name: Check if container already exists
      command: "pct status {{ lxc_id }}"
      register: container_status
      failed_when: false
      changed_when: false
      
    - name: Create LXC container
      command: |
        pct create {{ lxc_id }} local:vztmpl/{{ lxc_template }}.tar.zst \
          --hostname {{ lxc_hostname }} \
          --cores {{ lxc_cores }} \
          --memory {{ lxc_memory }} \
          --{{ lxc_network }} \
          --rootfs {{ lxc_storage }}:{{ lxc_disk_size }} \
          --features {{ lxc_features }} \
          --unprivileged 1 \
          --onboot 1 \
          --start 0
      when: container_status.rc != 0
      register: container_create
      
    - name: Configure GPU and TUN passthrough for container
      include_tasks: ../../tasks/proxmox_lxc_gpu_tun.yml
      vars:
        target_lxc_vmid: "{{ lxc_id }}"
        
    - name: Start container
      command: "pct start {{ lxc_id }}"
      register: container_start
      failed_when: false
      
    - name: Wait for container to be ready
      wait_for:
        host: "{{ ansible_default_ipv4.address }}"
        port: 22
        delay: 10
        timeout: 120
      delegate_to: localhost
      vars:
        ansible_connection: local
      when: container_start.rc == 0
      
    # Container configuration
    - name: Get container IP address
      shell: "pct exec {{ lxc_id }} -- ip -4 addr show eth0 | grep -oP '(?<=inet\\s)\\d+(\\.\\d+){3}' | head -n1"
      register: container_ip
      retries: 5
      delay: 10
      until: container_ip.stdout != ""
      
    - name: Display container IP
      debug:
        msg: "Container IP: {{ container_ip.stdout }}"
        
    # Bootstrap container
    - name: Update package cache in container
      command: "pct exec {{ lxc_id }} -- apt update"
      register: apt_update
      
    - name: Install required packages in container
      command: "pct exec {{ lxc_id }} -- apt install -y curl gnupg ca-certificates lsb-release software-properties-common apt-transport-https build-essential"
      
    # Docker installation
    - name: Check if Docker is already installed
      command: "pct exec {{ lxc_id }} -- which docker"
      register: docker_check
      failed_when: false
      changed_when: false
      
    - name: Add Docker GPG key in container
      command: "pct exec {{ lxc_id }} -- bash -c 'curl -fsSL https://download.docker.com/linux/debian/gpg | gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg'"
      when: docker_check.rc != 0
      
    - name: Add Docker repository in container
      command: |
        pct exec {{ lxc_id }} -- bash -c 'echo "deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian $(lsb_release -cs) stable" > /etc/apt/sources.list.d/docker.list'
      when: docker_check.rc != 0
        
    - name: Update package cache after adding Docker repository
      command: "pct exec {{ lxc_id }} -- apt update"
      when: docker_check.rc != 0
      
    - name: Install Docker in container
      command: "pct exec {{ lxc_id }} -- apt install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin"
      when: docker_check.rc != 0
      
    - name: Enable and start Docker service in container
      command: "pct exec {{ lxc_id }} -- systemctl enable --now docker"
      when: docker_check.rc != 0
      
    # NVIDIA container toolkit (no drivers needed in LXC - using passthrough)
    - name: Add NVIDIA Container Toolkit repository in container
      command: |
        pct exec {{ lxc_id }} -- bash -c 'curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg && echo "deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://nvidia.github.io/libnvidia-container/stable/deb/\$(ARCH) /" > /etc/apt/sources.list.d/nvidia-container-toolkit.list'
        
    - name: Update package cache for NVIDIA container toolkit
      command: "pct exec {{ lxc_id }} -- apt update"
      
    - name: Install NVIDIA Container Toolkit in container
      command: "pct exec {{ lxc_id }} -- apt install -y nvidia-container-toolkit"
      
    # Configure NVIDIA Container Toolkit for LXC
    - name: Deploy NVIDIA container runtime configuration
      template:
        src: "../../templates/lxc/nvidia-container-runtime-config.toml.j2"
        dest: "/tmp/nvidia-container-runtime-config.toml"
        mode: '0644'
        
    - name: Copy NVIDIA config to container
      command: "pct push {{ lxc_id }} /tmp/nvidia-container-runtime-config.toml /etc/nvidia-container-runtime/config.toml"
      
    - name: Configure Docker daemon for GPU support in container
      command: "pct exec {{ lxc_id }} -- nvidia-ctk runtime configure --runtime=docker"
      
    - name: Configure Docker daemon with external DNS servers
      command: |
        pct exec {{ lxc_id }} -- bash -c 'cat > /etc/docker/daemon.json << EOF
        {
            "runtimes": {
                "nvidia": {
                    "args": [],
                    "path": "nvidia-container-runtime"
                }
            },
            "dns": ["192.168.100.1", "8.8.8.8"]
        }
        EOF'
      
    - name: Restart Docker service in container
      command: "pct exec {{ lxc_id }} -- systemctl restart docker"
      
    # Deploy Ollama and Open WebUI
    - name: Create ollama directories in container
      command: "pct exec {{ lxc_id }} -- mkdir -p {{ docker_compose_dir }} {{ docker_compose_dir }}/data/ollama {{ docker_compose_dir }}/data/open-webui"
      
    - name: Deploy Docker Compose configuration
      template:
        src: "../../templates/lxc/ollama-docker-compose.yml.j2"
        dest: "/tmp/docker-compose.yml"
        mode: '0644'
        
    - name: Copy Docker Compose config to container
      command: "pct push {{ lxc_id }} /tmp/docker-compose.yml {{ docker_compose_dir }}/docker-compose.yml"
      
    - name: Start Ollama and Open WebUI services
      command: "pct exec {{ lxc_id }} -- bash -c 'cd {{ docker_compose_dir }} && docker compose up -d'"
      register: compose_start
      
    - name: Wait for Ollama service to be ready
      wait_for:
        host: "{{ container_ip.stdout }}"
        port: 11434
        delay: 30
        timeout: 180
      delegate_to: localhost
      when: compose_start.rc == 0
      
    # Pull initial models
    - name: Pull Llama 3.2 3B model
      command: "pct exec {{ lxc_id }} -- docker exec ollama ollama pull llama3.2:3b"
      async: 1800  # 30 minutes
      poll: 30
      register: llama_pull
      
    - name: Pull Mistral 7B model
      command: "pct exec {{ lxc_id }} -- docker exec ollama ollama pull mistral:7b"
      async: 1800  # 30 minutes  
      poll: 30
      register: mistral_pull
      when: llama_pull is succeeded
      
    # Firewall configuration
    - name: Configure firewall for Ollama ports
      blockinfile:
        path: "/etc/pve/nodes/{{ ansible_hostname }}/host.fw"
        marker: "# {mark} OLLAMA LXC PORTS"
        block: |
          [RULES]
          IN ACCEPT -i vmbr0 -p tcp -dport 11434 # Ollama API
          IN ACCEPT -i vmbr0 -p tcp -dport 3000  # Open WebUI
        create: yes
      notify: reload pve firewall
      
    - name: Add container to backup job
      blockinfile:
        path: "/etc/pve/jobs.cfg"
        marker: "# {mark} OLLAMA BACKUP"
        block: |
          vzdump: ollama-backup
            vmid 300
            storage local-pbs
            mode snapshot
            compress zstd
            mailnotification failure
            enabled 1
            schedule 'daily 03:00'
            prune-backups keep-daily=7,keep-weekly=4,keep-monthly=6
        create: yes
      failed_when: false
      
    # Final validation and information
    - name: Test GPU access in container
      command: "pct exec {{ lxc_id }} -- docker run --rm --gpus all nvidia/cuda:12.4-base-ubuntu22.04 nvidia-smi"
      register: gpu_test
      failed_when: false
      
    - name: Get service status
      command: "pct exec {{ lxc_id }} -- docker ps"
      register: docker_ps
      
    - name: Display setup completion information
      debug:
        msg:
          - "âœ… Ollama LXC container setup complete!"
          - ""
          - "Container Details:"
          - "  ID: {{ lxc_id }}"
          - "  Hostname: {{ lxc_hostname }}"
          - "  IP Address: {{ container_ip.stdout }}"
          - "  Resources: {{ lxc_cores }} cores, {{ lxc_memory }}MB RAM, {{ lxc_disk_size }}GB disk"
          - ""
          - "Services:"
          - "  Ollama API: http://{{ container_ip.stdout }}:11434"
          - "  Open WebUI: http://{{ container_ip.stdout }}:3000"
          - ""
          - "GPU Status: {{ 'Working' if gpu_test.rc == 0 else 'Check logs - may need troubleshooting' }}"
          - ""
          - "Docker Services:"
          - "{{ docker_ps.stdout_lines | default(['No services running']) | join('\n') }}"
          - ""
          - "Models Downloaded:"
          - "  - {{ 'llama3.2:3b' if llama_pull is succeeded else 'llama3.2:3b (failed)' }}"
          - "  - {{ 'mistral:7b' if mistral_pull is succeeded else 'mistral:7b (failed/skipped)' }}"
          - ""
          - "Next Steps:"
          - "  1. Access Open WebUI at http://{{ container_ip.stdout }}:3000"
          - "  2. Create your first user account"
          - "  3. Start chatting with your local LLMs!"

  handlers:
    - name: reload pve firewall
      command: pve-firewall restart
      listen: reload pve firewall