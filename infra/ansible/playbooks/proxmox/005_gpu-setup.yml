---
- name: GPU Setup for LXC Containers
  hosts: proxmox
  gather_facts: true
  become: true
  
  vars:
    nvidia_driver_install: true  # Set to false to skip driver installation
    use_repository: true  # Use NVIDIA CUDA repository instead of manual install
    
  tags:
    - gpu
    - drivers
    - lxc

  tasks:
    - name: Check for NVIDIA GPUs
      shell: lspci | grep -i nvidia
      register: nvidia_gpus
      changed_when: false
      failed_when: false
      tags: always
    
    - name: Display GPU information
      debug:
        msg: "{{ nvidia_gpus.stdout_lines }}"
      when: nvidia_gpus.rc == 0
      tags: always
    
    # Install NVIDIA drivers on host - Required for LXC passthrough
    - name: Install NVIDIA drivers for Proxmox
      when: nvidia_gpus.rc == 0 and nvidia_driver_install
      block:
        - name: Check if NVIDIA drivers are already installed
          command: nvidia-smi
          register: nvidia_smi_check
          failed_when: false
          changed_when: false

        - name: Blacklist nouveau driver
          when: nvidia_smi_check.rc != 0
          blockinfile:
            path: /etc/modprobe.d/blacklist-nouveau.conf
            create: yes
            block: |
              blacklist nouveau
              options nouveau modeset=0
          register: nouveau_blacklisted

        - name: Get current kernel version for Proxmox
          when: nvidia_smi_check.rc != 0
          command: uname -r
          register: kernel_version
          changed_when: false

        - name: Install required packages for NVIDIA driver
          when: nvidia_smi_check.rc != 0
          apt:
            name:
              - build-essential
              - dkms
              - pve-headers
              - gcc
              - make
            state: present
            update_cache: yes

        - name: Install kernel headers for current kernel
          when: nvidia_smi_check.rc != 0
          apt:
            name: "pve-headers-{{ kernel_version.stdout }}"
            state: present
          failed_when: false
          register: kernel_headers_install

        - name: Install generic Proxmox kernel headers if specific version failed
          when: nvidia_smi_check.rc != 0 and kernel_headers_install.failed
          apt:
            name: 
              - pve-headers
            state: present

        - name: Update initramfs if nouveau was blacklisted
          when: nouveau_blacklisted is changed
          command: update-initramfs -u
          
        - name: Check if reboot is needed for nouveau blacklist
          when: nouveau_blacklisted is changed
          set_fact:
            reboot_required: true
            
        - name: Add NVIDIA CUDA repository GPG key
          when: nvidia_smi_check.rc != 0 and use_repository
          get_url:
            url: https://developer.download.nvidia.com/compute/cuda/repos/debian12/x86_64/3bf863cc.pub
            dest: /usr/share/keyrings/cuda-archive-keyring.gpg
            mode: '0644'
            
        - name: Add NVIDIA CUDA repository
          when: nvidia_smi_check.rc != 0 and use_repository
          apt_repository:
            repo: "deb [signed-by=/usr/share/keyrings/cuda-archive-keyring.gpg] https://developer.download.nvidia.com/compute/cuda/repos/debian12/x86_64/ /"
            state: present
            filename: cuda
            
        - name: Update apt cache for CUDA repository
          when: nvidia_smi_check.rc != 0 and use_repository
          apt:
            update_cache: yes
            
        - name: Install NVIDIA driver and CUDA from repository
          when: nvidia_smi_check.rc != 0 and use_repository
          apt:
            name:
              - cuda-drivers
              - cuda-toolkit
            state: present
            install_recommends: yes
          register: nvidia_installed

        - name: Load NVIDIA kernel modules
          when: nvidia_installed is changed
          modprobe:
            name: "{{ item }}"
            state: present
          loop:
            - nvidia
            - nvidia_uvm
            - nvidia_modeset
            - nvidia_drm
          failed_when: false
          
        - name: Ensure NVIDIA modules load on boot
          when: nvidia_installed is changed
          lineinfile:
            path: /etc/modules-load.d/nvidia.conf
            line: "{{ item }}"
            create: yes
          loop:
            - nvidia
            - nvidia_uvm
            - nvidia_modeset
            - nvidia_drm

        - name: Install NVIDIA persistence daemon
          when: nvidia_installed is changed
          apt:
            name: nvidia-persistenced
            state: present
            
        - name: Enable and start NVIDIA persistence service
          when: nvidia_installed is changed
          systemd:
            name: nvidia-persistenced
            enabled: yes
            state: started
          failed_when: false

        - name: Add NVIDIA device permissions in udev
          when: nvidia_installed is changed
          copy:
            content: |
              # NVIDIA GPU devices
              KERNEL=="nvidia", RUN+="/bin/bash -c '/usr/bin/nvidia-smi -L && /bin/chmod 0666 /dev/nvidia*'"
              KERNEL=="nvidia_uvm", RUN+="/bin/bash -c '/usr/bin/nvidia-modprobe -c0 -u && /bin/chmod 0666 /dev/nvidia-uvm*'"
              SUBSYSTEM=="module", KERNEL=="nvidia", RUN+="/usr/bin/nvidia-modprobe -m"
            dest: /etc/udev/rules.d/70-nvidia.rules
            mode: '0644'
          register: udev_rules_created

        - name: Reload udev rules
          when: udev_rules_created is changed
          shell: udevadm control --reload-rules && udevadm trigger

    # Install NVIDIA Container Toolkit on host for LXC containers
    - name: Install NVIDIA Container Toolkit on Proxmox host
      when: nvidia_gpus.rc == 0
      block:
        - name: Remove old NVIDIA Container Toolkit repository if exists
          file:
            path: "{{ item }}"
            state: absent
          loop:
            - /etc/apt/sources.list.d/nvidia-container-toolkit.list
            - /usr/share/keyrings/nvidia-container-toolkit-keyring.asc
            
        - name: Add NVIDIA Container Toolkit GPG key
          get_url:
            url: https://nvidia.github.io/libnvidia-container/gpgkey
            dest: /tmp/nvidia-container-toolkit.gpg
            force: yes
            
        - name: Dearmor GPG key
          shell: cat /tmp/nvidia-container-toolkit.gpg | gpg --batch --yes --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
          args:
            creates: /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
            
        - name: Add NVIDIA Container Toolkit repository
          copy:
            content: "deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://nvidia.github.io/libnvidia-container/stable/deb/amd64 /\n"
            dest: /etc/apt/sources.list.d/nvidia-container-toolkit.list
            mode: '0644'
            
        - name: Update apt cache for NVIDIA Container Toolkit
          apt:
            update_cache: yes
            
        - name: Install NVIDIA Container Toolkit
          apt:
            name: nvidia-container-toolkit
            state: present

    # Create helper script for LXC GPU setup
    - name: Create LXC GPU helper script
      when: nvidia_gpus.rc == 0
      copy:
        content: |
          #!/bin/bash
          # Helper script to configure GPU passthrough for LXC containers
          
          set -e
          
          if [ "$#" -ne 1 ]; then
              echo "Usage: $0 <container_id>"
              echo "Example: $0 110"
              exit 1
          fi
          
          CTID=$1
          CONFIG_FILE="/etc/pve/lxc/${CTID}.conf"
          
          if [ ! -f "$CONFIG_FILE" ]; then
              echo "Error: Container $CTID not found"
              exit 1
          fi
          
          echo "Configuring GPU passthrough for container $CTID..."
          
          # Check if already configured
          if grep -q "lxc.cgroup2.devices.allow: c 195:\*" "$CONFIG_FILE"; then
              echo "GPU passthrough already configured for container $CTID"
              exit 0
          fi
          
          # Add GPU device permissions
          cat >> "$CONFIG_FILE" << 'EOF'
          
          # GPU Passthrough
          lxc.cgroup2.devices.allow: c 195:* rwm
          lxc.cgroup2.devices.allow: c 234:* rwm
          lxc.cgroup2.devices.allow: c 509:* rwm
          lxc.mount.entry: /dev/nvidia0 dev/nvidia0 none bind,optional,create=file
          lxc.mount.entry: /dev/nvidia1 dev/nvidia1 none bind,optional,create=file
          lxc.mount.entry: /dev/nvidiactl dev/nvidiactl none bind,optional,create=file
          lxc.mount.entry: /dev/nvidia-modeset dev/nvidia-modeset none bind,optional,create=file
          lxc.mount.entry: /dev/nvidia-uvm dev/nvidia-uvm none bind,optional,create=file
          lxc.mount.entry: /dev/nvidia-uvm-tools dev/nvidia-uvm-tools none bind,optional,create=file
          lxc.mount.entry: /dev/nvidia-caps dev/nvidia-caps none bind,optional,create=dir
          EOF
          
          echo "GPU passthrough configured. Restart container $CTID for changes to take effect."
          echo "Run: pct restart $CTID"
        dest: /usr/local/bin/lxc-gpu-passthrough
        mode: '0755'

    - name: Final GPU Status Check
      block:
        - name: Run nvidia-smi for final verification
          command: nvidia-smi
          register: final_nvidia_check
          changed_when: false
          failed_when: false
        
        - name: Check NVIDIA devices
          shell: ls -la /dev/nvidia* 2>/dev/null | head -10
          register: nvidia_devices
          changed_when: false
          failed_when: false
        
        - name: Display final GPU status
          debug:
            msg: |
              {% if reboot_required is defined and reboot_required %}
              ⚠️  REBOOT REQUIRED: Nouveau driver was blacklisted.
              Please reboot the system and run this playbook again.
              {% elif final_nvidia_check.rc == 0 %}
              ✅ NVIDIA drivers installed and working on host
              
              Devices available:
              {{ nvidia_devices.stdout }}
              
              Next steps for LXC containers:
              1. Use the helper script: lxc-gpu-passthrough <container_id>
              2. Inside container, install same NVIDIA driver version with --no-kernel-module flag
              
              GPU Status:
              {{ final_nvidia_check.stdout_lines[:10] | join('\n') }}
              {% else %}
              ⚠️  NVIDIA drivers not fully loaded. 
              Try: 
              1. Run 'nvidia-smi' to check status
              2. Check 'dmesg | grep -i nvidia' for errors
              3. Reboot may be required
              {% endif %}
      tags:
        - verify
  
  handlers:
    - name: update grub
      command: update-grub
      listen: "update grub"
    
    - name: reboot system
      reboot:
        msg: "Rebooting to apply GPU configuration changes"
        pre_reboot_delay: 10
        post_reboot_delay: 30
      listen: "reboot system"